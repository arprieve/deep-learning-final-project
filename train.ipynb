{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing the Model\n",
    "Lots of credit to the homework 1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import random\n",
    "\n",
    "DATA_PATH = 'simple_images'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.0, 1.0)\n",
    "])\n",
    "\n",
    "class ParksDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, path, train, transform=None):\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "                \n",
    "        # import the data now\n",
    "        self.data = []\n",
    "        dirs = os.listdir(path)\n",
    "        dirs.sort()\n",
    "        for item in dirs:\n",
    "            if not item.startswith(\".\"):\n",
    "                images = os.listdir(path + \"/\" + item)\n",
    "                count = int(len(images) * 0.8)\n",
    "                if train:\n",
    "                    images = images[:count]\n",
    "                else:\n",
    "                    images = images[count:]\n",
    "                    \n",
    "                for image in images:\n",
    "                    if not image.startswith(\".\"):\n",
    "                        label = item.replace(\"+\", \" \")[:-len(\" Landscape\")]\n",
    "                        self.data.append((path + \"/\" + item + \"/\" + image, label))  \n",
    "        random.shuffle(self.data)\n",
    "        \n",
    "        i = 0\n",
    "        self.label_to_idx = {}\n",
    "        for item in dirs:\n",
    "            if not item.startswith(\".\"):\n",
    "                label = item.replace(\"+\", \" \")[:-len(\" Landscape\")]\n",
    "                self.label_to_idx[label] = i\n",
    "                i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = norm_transform(io.imread(self.data[idx][0]))\n",
    "        label = self.label_to_idx[self.data[idx][1]]\n",
    "        return image, label\n",
    "    \n",
    "train_dataset = ParksDataset(csv_file=\"national-parks.csv\", path=\"simple_images\", train=True)\n",
    "test_dataset = ParksDataset(csv_file=\"national-parks.csv\", path=\"simple_images\", train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NatParkNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NatParkNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv5 = nn.Conv2d(128, 256, 2)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        self.norm4 = nn.BatchNorm2d(128)\n",
    "        self.norm5 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, len(train_dataset.label_to_idx))\n",
    "        self.accuracy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.maxpool(self.conv1(x))))\n",
    "        x = F.relu(self.norm2(self.maxpool(self.conv2(x))))\n",
    "        x = F.relu(self.norm3(self.maxpool(self.conv3(x))))\n",
    "        x = F.relu(self.norm4(self.maxpool(self.conv4(x))))\n",
    "        x = F.relu(self.norm5(self.avgpool(self.conv5(x))))\n",
    "        return self.fc1(torch.flatten(x, 1))\n",
    "\n",
    "    def loss(self, prediction, label, reduction='mean'):\n",
    "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
    "        return loss_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss(output, label)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                time.ctime(time.time()),\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return np.mean(losses)\n",
    "\n",
    "def test(model, device, test_loader, log_interval=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(test_loader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            test_loss_on = model.loss(output, label, reduction='sum').item()\n",
    "            test_loss += test_loss_on\n",
    "            pred = output.max(1)[1]\n",
    "            correct_mask = pred.eq(label.view_as(pred))\n",
    "            num_correct = correct_mask.sum().item()\n",
    "            correct += num_correct\n",
    "            if log_interval is not None and batch_idx % log_interval == 0:\n",
    "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    time.ctime(time.time()),\n",
    "                    batch_idx * len(data), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "num cpus: 16\n",
      "\n",
      "Test set: Average loss: 4.1108, Accuracy: 73/3840 (2%)\n",
      "\n",
      "Mon Dec 12 21:32:33 2022 Train Epoch: 0 [0/15253 (0%)]\tLoss: 4.184405\n",
      "\n",
      "Test set: Average loss: 3.2262, Accuracy: 734/3840 (19%)\n",
      "\n",
      "Mon Dec 12 21:36:23 2022 Train Epoch: 1 [0/15253 (0%)]\tLoss: 2.792121\n",
      "\n",
      "Test set: Average loss: 2.5733, Accuracy: 1291/3840 (34%)\n",
      "\n",
      "Mon Dec 12 21:40:16 2022 Train Epoch: 2 [0/15253 (0%)]\tLoss: 1.981238\n",
      "\n",
      "Test set: Average loss: 1.8119, Accuracy: 2251/3840 (59%)\n",
      "\n",
      "Mon Dec 12 21:44:05 2022 Train Epoch: 3 [0/15253 (0%)]\tLoss: 1.419944\n",
      "\n",
      "Test set: Average loss: 1.6644, Accuracy: 2078/3840 (54%)\n",
      "\n",
      "Mon Dec 12 21:47:51 2022 Train Epoch: 4 [0/15253 (0%)]\tLoss: 0.833485\n",
      "\n",
      "Test set: Average loss: 0.9260, Accuracy: 3110/3840 (81%)\n",
      "\n",
      "Mon Dec 12 21:51:42 2022 Train Epoch: 5 [0/15253 (0%)]\tLoss: 0.408472\n",
      "\n",
      "Test set: Average loss: 0.4315, Accuracy: 3621/3840 (94%)\n",
      "\n",
      "Mon Dec 12 21:55:32 2022 Train Epoch: 6 [0/15253 (0%)]\tLoss: 0.177558\n",
      "\n",
      "Test set: Average loss: 0.2435, Accuracy: 3746/3840 (98%)\n",
      "\n",
      "Mon Dec 12 21:59:19 2022 Train Epoch: 7 [0/15253 (0%)]\tLoss: 0.102242\n",
      "\n",
      "Test set: Average loss: 0.1682, Accuracy: 3773/3840 (98%)\n",
      "\n",
      "Mon Dec 12 22:03:08 2022 Train Epoch: 8 [0/15253 (0%)]\tLoss: 0.068084\n",
      "\n",
      "Test set: Average loss: 0.1471, Accuracy: 3791/3840 (99%)\n",
      "\n",
      "Mon Dec 12 22:07:03 2022 Train Epoch: 9 [0/15253 (0%)]\tLoss: 0.047671\n",
      "\n",
      "Test set: Average loss: 0.1050, Accuracy: 3798/3840 (99%)\n",
      "\n",
      "Mon Dec 12 22:10:57 2022 Train Epoch: 10 [0/15253 (0%)]\tLoss: 0.039048\n",
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 10\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "USE_CUDA = True\n",
    "SEED = 0\n",
    "PRINT_INTERVAL = 100\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "DATA_PATH = os.getcwd()\n",
    "\n",
    "EXPERIMENT_VERSION = \"0.1\"\n",
    "LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\n",
    "\n",
    "# Now the actual training code\n",
    "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print('Using device', device)\n",
    "import multiprocessing\n",
    "print('num cpus:', multiprocessing.cpu_count())\n",
    "\n",
    "kwargs = {'num_workers': multiprocessing.cpu_count(),\n",
    "          'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE,\n",
    "                                          shuffle=False, **kwargs)\n",
    "\n",
    "model = NatParkNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "start_epoch = 0\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_loss, test_accuracy = test(model, device, test_loader)\n",
    "\n",
    "test_losses.append((start_epoch, test_loss))\n",
    "test_accuracies.append((start_epoch, test_accuracy))\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    train_losses.append((epoch, train_loss))\n",
    "    test_losses.append((epoch, test_loss))\n",
    "    test_accuracies.append((epoch, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the model\n",
    "torch.save(model.state_dict(), 'my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show some results\n",
    "plt.plot(np.array(test_accuracies)[:,1])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.savefig(\"test_acc.png\", dpi=300, format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[294   0   0 ...   0   0   0]\n",
      " [  0 272   0 ...   0   0   0]\n",
      " [  0   0 264 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 280   0   0]\n",
      " [  0   0   0 ...   0 286   0]\n",
      " [  0   0   0 ...   0   0 284]]\n",
      "[100.         100.         100.         100.         100.\n",
      " 100.         100.         100.         100.         100.\n",
      " 100.         100.          98.94366197 100.         100.\n",
      " 100.         100.         100.         100.         100.\n",
      "  98.33887043 100.         100.         100.         100.\n",
      " 100.         100.         100.         100.          98.12030075\n",
      "  97.6744186  100.          96.77419355 100.         100.\n",
      " 100.         100.         100.         100.         100.\n",
      " 100.          99.64788732 100.         100.         100.\n",
      "  89.12280702 100.         100.         100.         100.\n",
      " 100.         100.          96.55172414 100.         100.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Some code to show a confusion matrix, turns out not very interesting with a good model\n",
    "true = torch.zeros(0, dtype=torch.long)\n",
    "pred = torch.zeros(0, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        true = torch.cat([true, classes.view(-1)])\n",
    "        pred = torch.cat([pred, predictions.view(-1)])\n",
    "\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "confusion = confusion_matrix(true.numpy(), pred.numpy())\n",
    "print(confusion)\n",
    "accuracy = 100 * confusion.diagonal() / confusion.sum(1)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse446",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6047351461a7bfefa99e8ad75da79d4d2ff134f5f72dd4ddfe8a8a9a477d009e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
